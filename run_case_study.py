import os
import json
import glob
import torch
from tqdm import tqdm
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from rank_bm25 import BM25Okapi
import re
import numpy as np

# ================= ğŸ”§ ä¸“ç”¨é…ç½® =================
# 1. åŸºç¡€è·¯å¾„ (ä½¿ç”¨ç»å¯¹è·¯å¾„ä»¥é˜²å‡ºé”™)
BASE_DIR = "/Users/nnn/Desktop/temp/åšå£«æ¯•ä¸š/ç¬¬å…­ç¯‡/elsarticle/experiment"
BI_ENCODER_PATH = "./cti_model_20k_finetuned"  # è¯·ç¡®ä¿è¿è¡Œç›®å½•ä¸‹æœ‰æ­¤æ–‡ä»¶å¤¹
CROSS_ENCODER_PATH = "new_experiment_4/cti_reranker_final"

# 2. çŸ¥è¯†åº“ä¸æŠ¥å‘Š
TECHNIQUE_DIR = "/Users/nnn/Desktop/temp/åšå£«æ¯•ä¸š/ç¬¬äº”ç¯‡/cti-master/enterprise-attack/attack-pattern/"
REPORTS_DIR = "generated_reports" # è¯·ç¡®ä¿è¿™æ˜¯ç›¸å¯¹äºè¿è¡Œç›®å½•çš„æ­£ç¡®è·¯å¾„

# 3. ç›®æ ‡æŠ¥å‘Š (åªæµ‹è¿™ä¸¤ä¸ª)
TARGET_REPORTS = [
    "report_352_20251222_151010_ground_truth.json", # Good Case
    "report_509_20251222_161855_ground_truth.json"  # Bad Case
]

# 4. LLM
LLM_API_KEY = "" 
LLM_BASE_URL = "https://api.deepseek.com"
client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

# å‚æ•°
TOP_K_RETRIEVE = 50   
TOP_K_RERANK = 10     

# ================= 1. ç³»ç»ŸåŠ è½½ =================
def load_system():
    print(">>> ğŸš€ Loading Case Study System...")
    
    bi_encoder = SentenceTransformer(BI_ENCODER_PATH)
    cross_encoder = CrossEncoder(CROSS_ENCODER_PATH)
    
    kb_texts = []
    kb_ids = []
    kb_info = {}
    kb_tokens = []
    
    json_files = glob.glob(os.path.join(TECHNIQUE_DIR, "*.json"))
    for fpath in json_files:
        try:
            with open(fpath, 'r', encoding='utf-8') as f:
                content = json.load(f)
            for obj in content.get('objects', []):
                if obj.get('type') != 'attack-pattern': continue
                is_ent = any(p.get('kill_chain_name')=='mitre-attack' for p in obj.get('kill_chain_phases',[]))
                if not is_ent or obj.get('x_mitre_deprecated') or obj.get('revoked'): continue
                
                tech_id = obj['external_references'][0]['external_id']
                name = obj['name']
                desc = obj['description']
                
                text = f"{name}: {desc}"
                kb_ids.append(tech_id)
                kb_texts.append(text)
                kb_tokens.append(f"{name} {desc} {tech_id}".lower().split())
                kb_info[tech_id] = {"name": name, "desc": desc}
        except: pass
        
    print(f"   âš¡ Encoded {len(kb_texts)} techniques.")
    kb_embs = bi_encoder.encode(kb_texts, convert_to_tensor=True)
    bm25 = BM25Okapi(kb_tokens)
    
    return bi_encoder, cross_encoder, kb_embs, bm25, kb_ids, kb_texts, kb_info

# ================= 2. æ¨ç†é€»è¾‘ =================

def analyze_chunk_advanced(text, bi_enc, cross_enc, kb_embs, bm25, kb_ids, kb_texts, kb_info):
    candidates_idx = set()
    
    # 1. Broad Retrieval
    w_emb = bi_enc.encode(text, convert_to_tensor=True)
    hits = util.semantic_search(w_emb, kb_embs, top_k=TOP_K_RETRIEVE)[0]
    for hit in hits: candidates_idx.add(hit['corpus_id'])
        
    b_scores = bm25.get_scores(text.lower().split())
    b_top = np.argsort(b_scores)[-TOP_K_RETRIEVE:]
    for i in b_top: candidates_idx.add(i)
        
    if not candidates_idx: return []
    
    # 2. Reranking
    cand_indices = list(candidates_idx)
    cross_inp = [[text, kb_texts[i]] for i in cand_indices]
    
    scores = cross_enc.predict(cross_inp)
    top_k_indices = np.argsort(scores)[-TOP_K_RERANK:]
    
    final_candidates = []
    for i in top_k_indices:
        final_candidates.append(kb_ids[cand_indices[i]])
        
    # 3. LLM Reasoning
    return llm_listwise_select(text, final_candidates, kb_info)

def llm_listwise_select(chunk_text, candidates, kb_info):
    cand_str = ""
    for idx, cid in enumerate(candidates):
        info = kb_info.get(cid, {})
        cand_str += f"Option {idx}: [ID: {cid}] {info.get('name')}\n"
        
    prompt = f"""
CTI Expert Task: Select ATT&CK techniques that strictly match the text.
Text: "{chunk_text}"
Options:
{cand_str}
Rules:
1. Select ONLY if the text describes specific malicious behavior matching the option.
2. If benign/generic, return empty.
Output JSON: {{ "indices": [0, 2] }}
"""
    try:
        resp = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={'type': 'json_object'},
            temperature=0.0
        )
        indices = json.loads(resp.choices[0].message.content).get("indices", [])
        return [candidates[i] for i in indices if 0 <= i < len(candidates)]
    except: return []

# ================= 3. è¾…åŠ©å·¥å…· =================
def get_sliding_windows(text):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    sentences = [s.strip() for s in sentences if len(s) > 10]
    windows = []
    for i in range(0, len(sentences), 1): 
        chunk = " ".join(sentences[i : i + 3])
        if len(chunk) > 20: windows.append(chunk)
    return windows

def get_parent_id(tid): return tid.split(".")[0] if "." in tid else tid

def check_match(pred, truth):
    if pred == truth: return True
    if get_parent_id(pred) == truth: return True
    if pred == get_parent_id(truth): return True
    return False

# ================= 4. è¯¦ç»†è¯Šæ–­æ‰“å° (æ ¸å¿ƒ) =================
def print_case_diagnosis(name, preds, true_ids, kb_info):
    pred_list = list(preds)
    true_list = list(true_ids)
    
    tp_set = set()
    fp_set = set()
    fn_set = set()
    covered_truths = set()
    
    # æ‰¾ TP (Match) å’Œ FP
    for p in pred_list:
        matched = False
        for t in true_list:
            if check_match(p, t):
                tp_set.add(p)
                covered_truths.add(t)
                matched = True
                break
        if not matched:
            fp_set.add(p)
            
    # æ‰¾ FN (Missed)
    for t in true_list:
        if t not in covered_truths:
            # åŒé‡æ£€æŸ¥
            is_covered = False
            for p in pred_list:
                if check_match(p, t):
                    is_covered = True; break
            if not is_covered:
                fn_set.add(t)
            else:
                covered_truths.add(t)

    # è®¡ç®—æŒ‡æ ‡
    p_val = len(tp_set)/len(pred_list) if pred_list else 0.0
    r_val = len(covered_truths)/len(true_list) if true_list else 0.0
    f1_val = 2*p_val*r_val/(p_val+r_val) if (p_val+r_val)>0 else 0.0
    
    print(f"\n{'='*20} ğŸ“ è¯Šæ–­æŠ¥å‘Š: {name} {'='*20}")
    print(f"ğŸ“Š F1: {f1_val:.2%} | Precision: {p_val:.2%} | Recall: {r_val:.2%}")
    print("-" * 60)
    
    print("âœ… TP (æˆåŠŸæ•è·):")
    if tp_set:
        for i in tp_set:
            info = kb_info.get(i, {'name': 'Unknown'})
            print(f"   [Prediction] {i:<12} -> {info['name']}")
    else: print("   (None)")

    print("\nâŒ FP (è¯¯æŠ¥ - é‡ç‚¹åˆ†æè¿™é‡Œ):")
    if fp_set:
        for i in fp_set:
            info = kb_info.get(i, {'name': 'Unknown'})
            print(f"   [Prediction] {i:<12} -> {info['name']}")
    else: print("   (None)")
        
    print("\nğŸ”» FN (æ¼æŠ¥ - é‡ç‚¹åˆ†æè¿™é‡Œ):")
    if fn_set:
        for i in fn_set:
            info = kb_info.get(i, {'name': 'Unknown'})
            print(f"   [Truth]      {i:<12} -> {info['name']}")
    else: print("   (None)")
        
    print("=" * 60 + "\n")

# ================= ä¸»ç¨‹åº =================
def run_case_study():
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(REPORTS_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æŠ¥å‘Šç›®å½• {REPORTS_DIR}")
        return

    bi_enc, cross_enc, kb_embs, bm25, kb_ids, kb_texts, kb_info = load_system()
    
    print(f"\n>>> ğŸš€ å¼€å§‹å¯¹ {len(TARGET_REPORTS)} ç¯‡ç›®æ ‡æŠ¥å‘Šè¿›è¡Œæ·±åº¦è¯Šæ–­...\n")
    
    for target_name in TARGET_REPORTS:
        r_path = os.path.join(REPORTS_DIR, target_name)
        if not os.path.exists(r_path):
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ°æ–‡ä»¶ {target_name}")
            continue
            
        with open(r_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        text = data.get('clean_text', '')
        true_ids = set(data.get('actual_found_ids', []))
        
        if not text: 
            print(f"âš ï¸ è·³è¿‡: {target_name} æ²¡æœ‰æ–‡æœ¬å†…å®¹")
            continue
        
        windows = get_sliding_windows(text)
        all_preds = set()
        
        # é€ä¸ªçª—å£æ‰«æ
        for w in tqdm(windows, desc=f"Scanning {target_name}", leave=False):
            ids = analyze_chunk_advanced(w, bi_enc, cross_enc, kb_embs, bm25, kb_ids, kb_texts, kb_info)
            all_preds.update(ids)
            
        # æ‰“å°è¯¦ç»†è¯Šæ–­
        print_case_diagnosis(target_name, all_preds, true_ids, kb_info)

if __name__ == "__main__":
    run_case_study()