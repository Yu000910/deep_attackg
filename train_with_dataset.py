import numpy as np
import torch
import os
import random
from collections import defaultdict
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# ================= é…ç½®åŒºåŸŸ =================
DATASET_PATH = "/Users/nnn/Desktop/temp/åšå£«æ¯•ä¸š/ç¬¬äº”ç¯‡/verb-tool-project/datasets/D_BEDR.npz"
MODEL_NAME = "BAAI/bge-base-en-v1.5"
OUTPUT_PATH = "cti_model_20k_finetuned"

# è®­ç»ƒå‚æ•° (M1 Pro ä¼˜åŒ–ç‰ˆ)
BATCH_SIZE = 16    # 20k æ•°æ®ï¼Œ16 æ¯”è¾ƒç¨³
EPOCHS = 4         # è®­ç»ƒ 4 è½®ï¼Œè¶³å¤Ÿæ”¶æ•›
LR = 2e-5

def load_and_balance_data(path):
    print(f">>> ğŸ“‚ Loading and Balancing Data from {path}...")
    data = np.load(path, allow_pickle=True)
    
    # 1. æå–æ–‡æœ¬å’Œæ ‡ç­¾
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ text å­˜å‚¨åœ¨ 'texts' key ä¸­ï¼Œå¦‚æœæŠ¥é”™è¯·æ”¹ä¸º keysåˆ—è¡¨ä¸­å®é™…çš„æ–‡æœ¬key
    texts = data['texts']
    labels = data['labels']
    
    # è§£ç  (å¦‚æœæ˜¯ bytes)
    decoded_texts = []
    for t in texts:
        if isinstance(t, bytes):
            decoded_texts.append(t.decode('utf-8'))
        else:
            decoded_texts.append(str(t))
            
    # 2. æŒ‰ç±»åˆ«åˆ†ç»„
    groups = defaultdict(list)
    for text, label in zip(decoded_texts, labels):
        groups[label].append(text)
        
    print(f"   Original: {len(decoded_texts)} samples, {len(groups)} classes.")
    
    # 3. è¿‡é‡‡æ · (Oversampling) - å…³é”®æ­¥éª¤
    # ç›®æ ‡ï¼šè®©æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°éƒ½è¾¾åˆ°æœ€å¤§ç±»çš„æ•°é‡ (æˆ–è€…æ˜¯ä¸­ä½æ•°ï¼Œè¿™é‡Œç”¨ 80 ä¹Ÿå°±æ˜¯æœ€å¤§å€¼)
    TARGET_COUNT = 80 
    balanced_pairs = []
    
    print(f"   âš–ï¸ Balancing classes to target count: {TARGET_COUNT}...")
    
    for label, samples in groups.items():
        # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œéšæœºé‡å¤é‡‡æ ·ç›´åˆ°å¡«æ»¡ TARGET_COUNT
        curr_samples = samples.copy()
        while len(curr_samples) < TARGET_COUNT:
            curr_samples.append(random.choice(samples)) # éšæœºå›é‡‡
            
        # å¦‚æœæ ·æœ¬æœ¬æ¥å°±å¾ˆå¤š(æ¯”å¦‚80)ï¼Œå°±æˆªæ–­æˆ–ä¿æŒ (è¿™é‡Œä¿æŒ)
        # ç°åœ¨æ„å»ºè®­ç»ƒå¯¹ (Anchor, Positive)
        # ä»åŒä¸€ä¸ªç±»é‡Œéšæœºé€‰ä¸¤ä¸ªä¸åŒçš„å¥å­ç»„æˆä¸€å¯¹
        for _ in range(TARGET_COUNT): 
            # éšæœºæŠ½ä¸¤ä¸ª
            a = random.choice(curr_samples)
            b = random.choice(curr_samples)
            # å°½é‡ä¸è¦è‡ªå·±å’Œè‡ªå·±é…å¯¹ï¼Œé™¤éåªæœ‰ä¸€æ¡æ•°æ®
            if a == b and len(set(curr_samples)) > 1:
                while b == a:
                    b = random.choice(curr_samples)
            
            balanced_pairs.append(InputExample(texts=[a, b]))
            
    print(f"âœ… Data Prepared. Total Training Pairs: {len(balanced_pairs)}")
    return balanced_pairs

def train():
    # 1. è®¾å¤‡æ£€æŸ¥
    if torch.backends.mps.is_available():
        device = "mps"
        print(">>> ğŸš€ MPS Acceleration Enabled")
    else:
        device = "cpu"
        print(">>> âš ï¸ Using CPU")

    # 2. å‡†å¤‡æ•°æ®
    train_examples = load_and_balance_data(DATASET_PATH)
    
    # åˆ‡åˆ†ä¸€å°éƒ¨åˆ†åšéªŒè¯ (å¯é€‰ï¼Œè¿™é‡Œä¸ºäº†æœ€å¤§åŒ–è®­ç»ƒæ•°æ®ï¼Œå…¨é‡è®­ç»ƒ)
    random.shuffle(train_examples)
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE, num_workers=0)

    # 3. åŠ è½½åŸºåº§æ¨¡å‹
    print(f">>> Loading base model: {MODEL_NAME}...")
    model = SentenceTransformer(MODEL_NAME, device=device)
    model.max_seq_length = 512

    # 4. æŸå¤±å‡½æ•°
    # MultipleNegativesRankingLoss æ˜¯æ— ç›‘ç£/è‡ªç›‘ç£è®­ç»ƒçš„ç¥å™¨
    # å®ƒä¼šæŠŠ batch é‡Œå…¶ä»–å¯¹çš„å¥å­ä½œä¸ºè´Ÿæ ·æœ¬
    train_loss = losses.MultipleNegativesRankingLoss(model=model)

    # 5. å¼€å§‹è®­ç»ƒ
    print(f">>> ğŸ‹ï¸ Starting Fine-Tuning ({EPOCHS} epochs)...")
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=EPOCHS,
        warmup_steps=int(len(train_dataloader) * 0.1),
        output_path=OUTPUT_PATH,
        optimizer_params={'lr': LR},
        show_progress_bar=True,
        use_amp=False 
    )
    
    print(f"âœ… Model saved to: {OUTPUT_PATH}")
    print("ğŸ‘‰ Now you can use this model in your evaluation script!")

if __name__ == "__main__":
    train()